{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddeffed-30d3-4e65-92c4-0e1145b38747",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2111037-72b6-4a1c-abd2-bb8014adefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aad4ec-73e0-429f-9efe-afdd9f9f6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e944073-d765-4460-88b8-36d08c3f1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74589b44-10e8-48b6-8939-66c74cdeef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import IPython\n",
    "import sklearn\n",
    "from haversine import haversine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "\n",
    "\n",
    "def errors_computation(ground_truth, predictions):\n",
    "\tdistances = list()\n",
    "\tfor i in range(0,len(ground_truth)):\n",
    "\t\tground_truth_list = ground_truth[i].tolist()\n",
    "\t\tpredict_list = predictions[i].tolist()\n",
    "\t\th= haversine(tuple(ground_truth_list),tuple(predict_list))*1000  # multiplying by 1000 to transform from Km to m\n",
    "\t\tdistances.append(h)\n",
    "\treturn distances\n",
    "\n",
    "def haversine_mean_error_computation(ground_truth, predictions):\n",
    "\tdistances = errors_computation(ground_truth, predictions)\n",
    "\treturn statistics.mean(distances)\n",
    "\n",
    "\n",
    "def haversine_error_statistics_computation(ground_truth, predictions,statistical_metric='mean',percentile=50):\n",
    "\tdistances = errors_computation(ground_truth, predictions)\n",
    "\tif statistical_metric==\"mean\":\n",
    "\t\treturn statistics.mean(distances)\n",
    "\telif statistical_metric==\"median\":\n",
    "\t\treturn statistics.median(distances)\n",
    "\telif statistical_metric==\"percentile\" and (percentile>=0 or percentile<=100):\n",
    "\t\treturn np.percentile(distances,percentile)\n",
    "\telse:\n",
    "\t\treturn statistics.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef920f3c-6ed0-4aae-9631-5485c9f208a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a squeeze and excitation block\n",
    "import keras_cv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Activation\n",
    "from keras_cv.layers import SqueezeAndExcite2D\n",
    "from keras import backend as K\n",
    "\n",
    "def SqueezeExcitation(x):\n",
    "  layer = keras_cv.layers.SqueezeAndExcite2D(8, bottleneck_filters=None,\n",
    "  squeeze_activation=\"relu\", excite_activation=\"sigmoid\" )\n",
    "  return layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f652c-c706-4ea3-af23-0a776eda3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arusha (leakrelu) activation function by calling leakrelu keras layer\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def arusha(x):\n",
    "  layer = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "  return layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0454ba-eacf-4156-9d44-676ef7c12551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating activation function called sinP\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "\n",
    "def sinP(x):\n",
    "  return tf.math.sin(x) + 1.5*x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee658e-f9b9-4d05-91f6-e0844e9b5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Kiazi (ELU) activation function by calling elu keras layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "\n",
    "def kiazi(x):\n",
    "  layer=tf.keras.layers.ELU(alpha=1.0)\n",
    "  return layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde565e-c974-4947-b8c5-4b20c0cf3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras_cv.layers import SqueezeAndExcite2D\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Activation,BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "#from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import Callback, TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Input, Dense, Flatten, Lambda, Conv2D, Concatenate, Add, Multiply, Reshape, Dropout, Conv3D, LeakyReLU, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import Input\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, Input\n",
    "\n",
    "SelebeaSamples = [130429]\n",
    "\n",
    "def exponential_scheme(x,minimum_value,a=60):\n",
    "\tpositive_x= x-minimum_value\n",
    "\tnumerator = np.exp(positive_x.div(a))\n",
    "\tdenominator = np.exp(-minimum_value/a)\n",
    "\texponential_x = numerator/denominator\n",
    "\texponential_x = exponential_x * 1000  #facilitating calculations\n",
    "\ttransformed_x = exponential_x\n",
    "\treturn transformed_x\n",
    "\n",
    "def powed_scheme(x,minimum_value,b=1.1):\n",
    "\tpositive_x= x-minimum_value\n",
    "\tnumerator = positive_x.pow(b)\n",
    "\tdenominator = (-minimum_value)**(b)\n",
    "\tpowed_x = numerator/denominator\n",
    "\ttransformed_x = powed_x\n",
    "\treturn transformed_x\n",
    "\n",
    "def positive_scheme(x,minimum_value):\n",
    "  positive_x = x-minimum_value\n",
    "  transformed_x = positive_x\n",
    "  return transformed_x\n",
    "\n",
    "def normalized_scheme(x, minimum_value):\n",
    "  positive_x = x-minimum_value\n",
    "  normalized_x = positive_x/(-minimum_value)\n",
    "  transformed_x = normalized_x\n",
    "  return transformed_x\n",
    "\n",
    "\n",
    "# reading the data\n",
    "file = pd.read_csv(\"/content/drive/My Drive/LORAWAN_DATASETS/lorawan_antwerp_2019_dataset.csv\")\n",
    "albert = pd.read_csv(\"/content/drive/My Drive/LORAWAN_DATASETS/New2019Dataset.csv\")\n",
    "columns = file.columns\n",
    "columns_02 = albert.columns\n",
    "\n",
    "z = albert[columns_02[1:]]\n",
    "z = z.join(file[columns[75:]])\n",
    "\n",
    "#shuffling the dataset with 42 seeds\n",
    "sele = pd.DataFrame(z)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(sele.values)\n",
    "\n",
    "#Splitting the dataset to get training data and training labels\n",
    "columns_03 = sele.columns\n",
    "x = sele[columns_03[:45]]\n",
    "y = sele[columns_03[45:]]\n",
    "\n",
    "# Adjusting the minimum_value\n",
    "x = x.replace(-200,200)\n",
    "minimum_value = x.min().min() - 1\n",
    "x = x.replace(200,minimum_value)\n",
    "print('minimum_value')\n",
    "print(minimum_value)\n",
    "\n",
    "transformed_x = powed_scheme(x,minimum_value)\n",
    "#transformed_x = exponential_scheme(x,minimum_value)\n",
    "#transformed_x = positive_scheme(x,minimum_value)\n",
    "#transformed_x = normalized_scheme(x,minimum_value)\n",
    "\n",
    "\n",
    "#Defining my custom data split\n",
    "i = 0\n",
    "for i, SelebeaSample in enumerate(SelebeaSamples):\n",
    "\t# Training size\n",
    "  trainings_size = 0.7                     # 70% training set\n",
    "  validation_size = 0.15                     # 15% validation set\n",
    "  test_size = 0.15                          # 15% test set\n",
    "\n",
    "  data=transformed_x\n",
    "\n",
    "  print(SelebeaSample)\n",
    "\n",
    "  x_train = data[:int(trainings_size*SelebeaSample)] # first 70% of the data\n",
    "  x_val = data[int(trainings_size*SelebeaSample):int((trainings_size + validation_size) * SelebeaSample)]\n",
    "  x_test = data[-int(test_size * SelebeaSample):] # last 15% of the data\n",
    "\n",
    "  y_train = y[:int(trainings_size*SelebeaSample)] # first 70% of the data\n",
    "  y_val = y[int(trainings_size*SelebeaSample):int((trainings_size + validation_size) * SelebeaSample)]\n",
    "  y_test = y[-int(test_size * SelebeaSample):] # last 15% of the data\n",
    "\n",
    "\n",
    "'''Scaling data values using MinMaxScaler to ensure that the data values are within a fixed range\n",
    "and contributes equally to the analysis (MinMaxScaler is useful when the data has a bounded range\n",
    "or when the distribution is not Gaussian)'''\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "scaler_y = preprocessing.MinMaxScaler().fit(y_train)\n",
    "y_train = scaler_y.transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# Make an instance of the Model\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(x_train)\n",
    "# Determining the number of components PCA has fitted\n",
    "N=pca.n_components_\n",
    "\n",
    "# Calculate Variance Explained\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate Cumulative Variance Explained\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "print(var_exp)\n",
    "#print(cum_var_exp)\n",
    "print(N)\n",
    "\n",
    "# Applying the mapping (Transform) to the training, validation and test set\n",
    "x_train = pca.transform(x_train)\n",
    "x_val = pca.transform(x_val)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "a=x_train.shape[0]\n",
    "b=x_val.shape[0]\n",
    "c=x_test.shape[0]\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#changing shape of the input data to be compatible with conv2D layers\n",
    "x_train=x_train.reshape((a,1,45,1))\n",
    "x_val=x_val.reshape((b,1,45,1))\n",
    "x_test=x_test.reshape((c,1,45,1))\n",
    "\n",
    "#cumulative explained variance ratio as a function of the number of components:\n",
    "\n",
    "#plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "#plt.xlabel('number of components')\n",
    "#plt.ylabel('cumulative explained variance')\n",
    "\n",
    "dropout = 0.15\n",
    "#l2 = 0.00\n",
    "learning_rate = 0.001\n",
    "epochs = 150\n",
    "batch_size= 512\n",
    "patience = 100\n",
    "\n",
    "''' Defining the architecture of the CNN model '''\n",
    "\n",
    "input_tensor = Input(shape=(1, 45, 1))\n",
    "\n",
    "branch_a = Conv2D(8, (1, 1), padding='same', kernel_regularizer=regularizers.l1(0.01))(input_tensor)\n",
    "branch_a = SqueezeExcitation(branch_a)\n",
    "branch_a = arusha(branch_a)\n",
    "branch_a = Dropout(dropout)(branch_a)\n",
    "\n",
    "branch_b = Conv2D(8, (1, 1), padding='same')(input_tensor)\n",
    "branch_b = SqueezeExcitation(branch_b)\n",
    "branch_b = arusha(branch_b)\n",
    "branch_b = Dropout(dropout)(branch_b)\n",
    "branch_b = Conv2D(8, (1, 1), padding='same', kernel_regularizer=regularizers.l1(0.01))(branch_b)\n",
    "branch_b = SqueezeExcitation(branch_b)\n",
    "branch_b = arusha(branch_b)\n",
    "\n",
    "branch_c = Conv2D(8, (1, 1), padding='same')(input_tensor)\n",
    "branch_c = SqueezeExcitation(branch_c)\n",
    "branch_c = arusha(branch_c)\n",
    "branch_c = Dropout(dropout)(branch_c)\n",
    "branch_c = Conv2D(8, (1, 1), padding='same')(branch_c)\n",
    "branch_c = SqueezeExcitation(branch_c)\n",
    "branch_c = arusha(branch_c)\n",
    "branch_c = Dropout(dropout)(branch_c)\n",
    "branch_c = Conv2D(8, (1, 1), padding='same', kernel_regularizer=regularizers.l1(0.01))(branch_c)\n",
    "branch_c = SqueezeExcitation(branch_c)\n",
    "branch_c = arusha(branch_c)\n",
    "\n",
    "output = Concatenate()([branch_a, branch_b, branch_c])\n",
    "\n",
    "output_tensor= Flatten()(output)\n",
    "output_tensor = Dense(512, activation='relu')(output_tensor)\n",
    "output_tensor = Dense(256, activation='relu')(output_tensor)\n",
    "output_tensor = Dense(128, activation='relu')(output_tensor)\n",
    "output_tensor = Dense(64, activation='relu')(output_tensor)\n",
    "output_tensor = Dense(32, activation='relu')(output_tensor)\n",
    "output_tensor = Dense(2, activation='linear')(output_tensor)\n",
    "MyModel = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "MyModel.compile(optimizer=Adam(learning_rate=learning_rate), loss='mae', metrics=['mae'])\n",
    "MyModel.summary()\n",
    "\n",
    "cb =[EarlyStopping(monitor='val_loss', mode='min', patience=patience, verbose =1, restore_best_weights=True)]\n",
    "dlr = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)]\n",
    "mc = ModelCheckpoint('bestmodels/best_model_lorawan_' + str(SelebeaSample) + '.h5', monitor='val_mae', mode='min', verbose=1, save_best_only=True)\n",
    "#history = MyModel.fit(x_train, y_train,validation_data=(x_val, y_val),epochs=epochs, batch_size=batch_size, verbose=1, callbacks= [cb,mc])\n",
    "history = MyModel.fit(x_train, y_train,validation_data=(x_val, y_val),epochs=epochs, batch_size=batch_size, verbose=1, callbacks= [cb,dlr,mc])\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Train and Validation Errors During Model Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.savefig(\"/content/drive/My Drive/RESULTS/training_curves.png\")\n",
    "plt.show()\n",
    "\n",
    "MyModel = load_model('bestmodels/best_model_lorawan_' + str(SelebeaSample)  + '.h5')\n",
    "y_predict = MyModel.predict(x_test, batch_size=batch_size)\n",
    "y_predict_in_val = MyModel.predict(x_val, batch_size=batch_size)\n",
    "y_predict_in_train = MyModel.predict(x_train, batch_size=batch_size)\n",
    "\n",
    "\n",
    "y_predict = scaler_y.inverse_transform(y_predict)\n",
    "pd.DataFrame(y_predict).to_csv(\"/content/drive/My Drive/RESULTS/predicted_y_values.csv\")\n",
    "\n",
    "\n",
    "\n",
    "y_predict_in_train = scaler_y.inverse_transform(y_predict_in_train)\n",
    "y_predict_in_val = scaler_y.inverse_transform(y_predict_in_val)\n",
    "y_train = scaler_y.inverse_transform(y_train)\n",
    "y_val = scaler_y.inverse_transform(y_val)\n",
    "\n",
    "y_test = scaler_y.inverse_transform(y_test)\n",
    "pd.DataFrame(y_test).to_csv(\"/content/drive/My Drive/RESULTS/true_y_values.csv\")\n",
    "\n",
    "\n",
    "#Calculating R2 score\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "print('r2 score for this model is', r2)\n",
    "\n",
    "print(\"Train set mean error: {:.2f}\".format(haversine_error_statistics_computation(y_predict_in_train, y_train,'mean')))\n",
    "print(\"Train set median error: {:.2f}\".format(haversine_error_statistics_computation(y_predict_in_train, y_train,'median')))\n",
    "print(\"Train set 75th perc error: {:.2f}\".format(haversine_error_statistics_computation(y_predict_in_train, y_train,'percentile',75)))\n",
    "print(\"Val set mean error: {:.2f}\".format(haversine_error_statistics_computation(y_predict_in_val, y_val,'mean')))\n",
    "print(\"Val set median error: {:.2f}\".format(haversine_error_statistics_computation(y_predict_in_val, y_val,'median')))\n",
    "print(\"Val set 75th perc.  error: {:.2f}\".format(haversine_error_statistics_computation(y_predict_in_val, y_val,'percentile',75)))\n",
    "print(\"Test set mean error: {:.2f}\".format(haversine_error_statistics_computation(y_predict, y_test,'mean')))\n",
    "print(\"Test set median error: {:.2f}\".format(haversine_error_statistics_computation(y_predict, y_test,'median')))\n",
    "print(\"Test set  75th perc. error: {:.2f}\".format(haversine_error_statistics_computation(y_predict, y_test,'percentile',75)))\n",
    "\n",
    "errors = errors_computation(y_predict,y_test)\n",
    "pd.DataFrame(errors).to_csv(\"/content/drive/My Drive/RESULTS/errors.csv\")\n",
    "print(\"end of experiment!!!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
